{"cells":[{"cell_type":"markdown","source":["#What is Decision Tree?\n![Decision_Tree](https://raw.githubusercontent.com/PolinaRus/My_Projects/master/Decision%20Tree.png)\n\n#Decision Tree\nDecision trees are a very straightforward machine learning model, and visualizing generic tree structures is a fairly well established technique."],"metadata":{}},{"cell_type":"markdown","source":["#Binary Classification\n**Binary Classification** is the task of predicting a binary label. E.g., is an email spam or not spam? Should I show this ad to this user or not? Will it rain tomorrowor not? This section demonstrates algorithms for making these types of predictions."],"metadata":{}},{"cell_type":"markdown","source":["# Data\n\nThe Country Life Expectancy dataset we are going to use consists of information about 152 countries and their average Life Expectancy\nWe will use this information to predict if an average age of a country is <65 or >=65. The dataset is rather clean, and consists of both numeric and categorical variables.\n\n**Attribute Information:**\n\n* **Country:** Country name\n* **Code:** Country code\n* **Continent:** Continent that the country is located\n* **Religion:** Main religion practiced in the country\n* **Status:** Whether the country is developed, developing or poor\n* **Population:** 2012 population of the country\n* **Labor Forece:** 2012 labor force of the country\n* **GDP:** 2012 GDP per capita of the country\n* **Urbanization:** 2012 urbanization % of the country\n* **Literacy:** 2012 literacy % of the country\n* **Population Growth Rate:** 2012 population growth rate % of the country\n* **Below Poverty Line:** 2012 population below poverty line % of the country\n* **Median Age:** 2012 median age of the country\n* **Life Expectancy:** 2012 average life expectancy of the country\n* **Age:** Whether the country's average life expectancy is below or above 65\n\n**Target/Label:** < 65, >= 65"],"metadata":{}},{"cell_type":"markdown","source":["# Load Data\nIn this example, we will read in the Life Expectancy dataset in SQL using the CSV data source for Spark and rename the columns appropriately."],"metadata":{}},{"cell_type":"code","source":["%sh \nmkdir -p life_pred\ncurl 'https://raw.githubusercontent.com/PolinaRus/CIS_8795/master/Life_Expectancy.csv' > life_pred/life_expectancy.csv\nls /databricks/driver/life_pred"],"metadata":{},"outputs":[],"execution_count":5},{"cell_type":"code","source":["%fs \nls file:/databricks/driver/life_pred"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"code","source":["%sql DROP TABLE IF EXISTS life_expectancy"],"metadata":{},"outputs":[],"execution_count":7},{"cell_type":"code","source":["%sql\n\nCREATE TABLE life_expectancy (\n  Country STRING,\n  Code STRING,\n  Continent STRING,\n  Religion STRING,\n  Status STRING,\n  Population DOUBLE,\n  Labor_Force DOUBLE,\n  GDP DOUBLE,\n  Urbanization DOUBLE,\n  Literacy DOUBLE,\n  Population_Growth_Rate DOUBLE,\n  Below_Poverty_Line DOUBLE,\n  Median_Age DOUBLE,\n  Life_Expectancy DOUBLE,\n  Age STRING)\nUSING com.databricks.spark.csv\nOPTIONS (path \"file:/databricks/driver/life_pred/life_expectancy.csv\", header \"true\")"],"metadata":{},"outputs":[],"execution_count":8},{"cell_type":"code","source":["my_data = spark.table(\"life_expectancy\")\ncols = my_data.columns"],"metadata":{},"outputs":[],"execution_count":9},{"cell_type":"code","source":["display(my_data)"],"metadata":{},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["# Process Data\nFor Decision Tree analysis we need to convert the categorical variables in the dataset into numeric variables. There are 2 ways we can do this.\n\n* **Category Indexing.**\nThis is basically assigning a numeric value to each category from {0, 1, 2, ...numCategories-1}. This introduces an implicit ordering among your categories, and is more suitable for ordinal variables (eg: Poor: 0, Average: 1, Good: 2)\n\n* **One-Hot Encoding.**\nThis converts categories into binary vectors with at most one nonzero value (eg: (Blue: [1, 0]), (Green: [0, 1]), (Red: [0, 0]))\n\nHere, we will use a combination of StringIndexer and OneHotEncoder to convert the categorical variables. The OneHotEncoder will return a SparseVector.\n\nSince we will have more than 1 stages of feature transformations, we use a Pipeline to tie the stages together. This simplifies our code."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml import Pipeline\nfrom pyspark.ml.feature import OneHotEncoder, StringIndexer, VectorAssembler\n\ncategoricalColumns = [\"Country\", \"Code\", \"Continent\", \"Religion\", \"Status\"]\nstages = [] \nfor categoricalCol in categoricalColumns:\n\n  stringIndexer = StringIndexer(inputCol=categoricalCol, outputCol=categoricalCol+\"Index\")\n\n  encoder = OneHotEncoder(inputCol=categoricalCol+\"Index\", outputCol=categoricalCol+\"classVec\")\n\n  stages += [stringIndexer, encoder]"],"metadata":{},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["The above code basically indexes each categorical column using the StringIndexer, and then converts the indexed categories into one-hot encoded variables. The resulting output has the binary vectors appended to the end of each row.\n\nWe use the StringIndexer again here to encode our labels to label indices."],"metadata":{}},{"cell_type":"code","source":["label_stringIdx = StringIndexer(inputCol = \"Age\", outputCol = \"label\")\nstages += [label_stringIdx]"],"metadata":{},"outputs":[],"execution_count":14},{"cell_type":"markdown","source":["Next, we will use the VectorAssembler to combine all the feature columns into a single vector column. This will include both the numeric columns and the one-hot encoded binary vector columns in our dataset."],"metadata":{}},{"cell_type":"code","source":["numericCols = [\"Population\", \"Labor_Force\", \"GDP\", \"Urbanization\", \"Literacy\", \"Population_Growth_Rate\",\"Below_Poverty_Line\",\"Median_Age\", \"Life_Expectancy\"]\nassemblerInputs = map(lambda c: c + \"classVec\", categoricalColumns) + numericCols\nassembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\nstages += [assembler]"],"metadata":{},"outputs":[],"execution_count":16},{"cell_type":"markdown","source":["We finally run our stages as a Pipeline. This puts the data through all of the feature transformations we described in a single call."],"metadata":{}},{"cell_type":"code","source":["pipeline = Pipeline(stages=stages)\n\nmy_Model = pipeline.fit(my_data)\ndf = my_Model.transform(my_data)\n\nselectedcols = [\"label\", \"features\"] + cols\ndf = df.select(selectedcols)\ndisplay(df)"],"metadata":{},"outputs":[],"execution_count":18},{"cell_type":"markdown","source":["Randomly split data into training and test sets. set seed for reproducibility"],"metadata":{}},{"cell_type":"code","source":["(trainingData, testData) = df.randomSplit([0.7, 0.3], seed = 100)\nprint trainingData.count()\nprint testData.count()"],"metadata":{},"outputs":[],"execution_count":20},{"cell_type":"markdown","source":["#Fit and Evaluate Models\n\nWe are now ready to try out some of the Binary Classification algorithms available in the Pipelines API.\n\nOut of these algorithms, the below are also capable of supporting multiclass classification with the Python API: - Decision Tree Classifier - Random Forest Classifier\n\nThese are the general steps we will take to build our models: - Create initial model using the training set - Tune parameters with a ParamGrid and 5-fold Cross Validation - Evaluate the best model obtained from the Cross Validation using the test set\n\nWe will be using the BinaryClassificationEvaluator to evaluate our models. The default metric used here is areaUnderROC."],"metadata":{}},{"cell_type":"markdown","source":["# Decision Trees\nThe Decision Trees algorithm is popular because it handles categorical data and works out of the box with multiclass classification tasks."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.classification import DecisionTreeClassifier\n\n# Create initial Decision Tree Model\ndt = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\", maxDepth=3)\n\n# Train model with Training Data\ndtModel = dt.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":23},{"cell_type":"markdown","source":["We can extract the number of nodes in our decision tree as well as the tree depth of our model."],"metadata":{}},{"cell_type":"code","source":["print \"numNodes = \", dtModel.numNodes\nprint \"depth = \", dtModel.depth"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Make predictions on test data using the Transformer.transform() method."],"metadata":{}},{"cell_type":"code","source":["predictions = dtModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":27},{"cell_type":"markdown","source":["Print the schema of the prediction table"],"metadata":{}},{"cell_type":"code","source":["predictions.printSchema()"],"metadata":{},"outputs":[],"execution_count":29},{"cell_type":"markdown","source":["View model's predictions and probabilities of each prediction class"],"metadata":{}},{"cell_type":"code","source":["selected = predictions.select(\"label\", \"prediction\", \"probability\", \"Life_Expectancy\", \"Status\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":31},{"cell_type":"markdown","source":["We will evaluate our Decision Tree model with BinaryClassificationEvaluator."],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.evaluation import BinaryClassificationEvaluator\n\nevaluator = BinaryClassificationEvaluator()\nevaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":33},{"cell_type":"markdown","source":["Create ParamGrid for Cross Validation"],"metadata":{}},{"cell_type":"code","source":["from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n\nparamGrid = (ParamGridBuilder()\n             .addGrid(dt.maxDepth, [1,2,6,10])\n             .addGrid(dt.maxBins, [20,40,80])\n             .build())"],"metadata":{},"outputs":[],"execution_count":35},{"cell_type":"markdown","source":["Create 5-fold CrossValidator and run cross validation"],"metadata":{}},{"cell_type":"code","source":["cv = CrossValidator(estimator=dt, estimatorParamMaps=paramGrid, evaluator=evaluator, numFolds=5)\ncvModel = cv.fit(trainingData)"],"metadata":{},"outputs":[],"execution_count":37},{"cell_type":"code","source":["print \"numNodes = \", cvModel.bestModel.numNodes\nprint \"depth = \", cvModel.bestModel.depth"],"metadata":{},"outputs":[],"execution_count":38},{"cell_type":"markdown","source":["Use test set here so we can measure the accuracy of our model on new data. cvModel uses the best model found from the Cross Validation"],"metadata":{}},{"cell_type":"code","source":["predictions = cvModel.transform(testData)"],"metadata":{},"outputs":[],"execution_count":40},{"cell_type":"markdown","source":["Evaluate the best model"],"metadata":{}},{"cell_type":"code","source":["evaluator.evaluate(predictions)"],"metadata":{},"outputs":[],"execution_count":42},{"cell_type":"markdown","source":["View Best model's predictions and probabilities of each prediction class"],"metadata":{}},{"cell_type":"code","source":["selected = predictions.select(\"label\", \"prediction\", \"probability\", \"Age\", \"Status\")\ndisplay(selected)"],"metadata":{},"outputs":[],"execution_count":44},{"cell_type":"markdown","source":["Evaluate the accuracy of the Decision Tree model"],"metadata":{}},{"cell_type":"code","source":["accuracy = evaluator.evaluate(predictions)\nprint(\"Test Error = %g\" % (1.0 - accuracy))\nprint(\"Accuracy= %g\" % (accuracy))"],"metadata":{},"outputs":[],"execution_count":46},{"cell_type":"markdown","source":["As shown above, our Decision Tree model is 100% accurate"],"metadata":{}}],"metadata":{"name":"My_Decision Tree","notebookId":575440804464750},"nbformat":4,"nbformat_minor":0}
